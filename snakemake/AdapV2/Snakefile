import pandas as pd

metadata_file = config["input_meta"]
metadata = pd.read_csv(metadata_file, sep="\t").set_index(["batch","sample"])
nparallel = config['nparallel'] if 'nparallel' in config else 40
tmpdir=config['tmpdir']


batch_ids, sample_names = list(zip(*metadata.index))
batch_ids = set(batch_ids)
sample_names = set(sample_names)
print("batch_ids", batch_ids)
print("sample_names", sample_names)

SPLIT_OUT="split-out"
SPLITS = [str(x+1) for x in range(nparallel)]

REF= config['ref']
CODEC_root = config['codec_root']

DEMUX=f"{CODEC_root}/cmake-build-release/codec demux"
TRIM=f"{CODEC_root}/cmake-build-release/codec trim"
CONSENSUS_BIN=f"{CODEC_root}/cmake-build-release/codec consensus"
ACCU_BIN=f"{CODEC_root}/cmake-build-release/codec accuracy"
AGG_METRICS_SCRIPT=f"{CODEC_root}/snakemake/script/cds_summarize.py"
AGG_LOG_SCRIPT=f"{CODEC_root}/snakemake/script/agg_log.py"
SPLIT_SCRIPT=f"{CODEC_root}/snakemake/script/fastqsplit.pl"
MONOMER_ERROR_SCRIPT = f"{CODEC_root}/snakemake/script/calculate_monomer_error_rate.py"
UPDATE_MET= f"{CODEC_root}/snakemake/script/updatemet"
CODEC2MAF= f"{CODEC_root}/snakemake/script/codec2maf"

JAVA_PARAMS = "-Xmx30g -Djava.io.tmpdir=/broad/hptmp"
BWA = config['bwa']
PICARD = f"java {JAVA_PARAMS} -jar $PICARD"
GATK = f"{config['gatk4']} --java-options \"{JAVA_PARAMS}\""
GATK3 = f"{config['gatk3']} --java-options \"{JAVA_PARAMS}\""
FGBIO = f"java {JAVA_PARAMS} -jar {config['fgbio']}"

DETECT_DIR="detect"
ACCU_OUT="accu_out"
ADAPTER_TRIM_OUT="adap_trimmed"
TRIM_MAPPED="mapped"
Metrics_OUT="metrics"

workdir: config["cwd"]

batch_to_fastq = metadata.reset_index().groupby('batch').agg({'fastq1' : set, 'fastq2': set})
batch_to_samplesheet = metadata.reset_index().groupby('batch').agg({'sample_sheet' : set})
batch_to_samples = metadata.reset_index().groupby('batch').agg({'sample' : set})
sample_to_batch = metadata.reset_index().groupby('sample').agg({'batch' : set})
#sample_to_vcf = metadata.reset_index().groupby('sample').agg({'germline_vcf' : set})
sample_to_germbam = metadata.reset_index().groupby('sample').agg({'germline_bam' : set})
rule SplitFastq1:
    input:
         lambda wildcards: batch_to_fastq.loc[wildcards.id]['fastq1']
    params:
          nsplit = nparallel,
          prefix = "/broad/hptmp/ruolin/{id}_split_r1"
    output:
          split = temp(expand("/broad/hptmp/ruolin/{{id}}_split_r1.{ss}.fastq", ss = SPLITS))
    resources:
        mem = 8,
        runtime = 24
    shell:
        """
        zcat {input} | {SPLIT_SCRIPT} {params.prefix} {params.nsplit}
        """

rule SplitFastq2:
    input:
         lambda wildcards: batch_to_fastq.loc[wildcards.id]['fastq2']
    params:
          nsplit = nparallel,
          prefix = "/broad/hptmp/ruolin/{id}_split_r2"
    output:
          split = temp(expand("/broad/hptmp/ruolin/{{id}}_split_r2.{ss}.fastq", ss = SPLITS))
    resources:
        mem = 8,
        runtime = 24
    shell:
         """
         zcat {input} | {SPLIT_SCRIPT} {params.prefix} {params.nsplit}
         """

rule DemuxL1:
    input:
      read1 = "/broad/hptmp/ruolin/L001_split_r1.{ss}.fastq",
      read2 = "/broad/hptmp/ruolin/L001_split_r2.{ss}.fastq",
      sample_sheet = lambda wildcards: batch_to_samplesheet.loc['L001']['sample_sheet']
    output:
      read1 = temp(expand("demux/L001_split.{{ss}}.{index}.1.fastq.gz", index = batch_to_samples.loc['L001']['sample'])),
      read2 = temp(expand("demux/L001_split.{{ss}}.{index}.2.fastq.gz",  index = batch_to_samples.loc['L001']['sample'])),
    resources:
     mem = 8,
     runtime = 24
    wildcard_constraints:
       ss = "[0-9]+"
    params:
     outprefix = "demux/L001_split.{ss}",
     log = "demux/L001_split.{ss}.log",
     ref = {REF}
    shell:
     """
         {DEMUX} -1 {input.read1} -2 {input.read2} -p {input.sample_sheet} -o {params.outprefix} > {params.log}
     """

rule DemuxL2:
    input:
      read1 = "/broad/hptmp/ruolin/L002_split_r1.{ss}.fastq",
      read2 = "/broad/hptmp/ruolin/L002_split_r2.{ss}.fastq",
      sample_sheet = lambda wildcards: batch_to_samplesheet.loc['L002']['sample_sheet']
    output:
      read1 = temp(expand("demux/L002_split.{{ss}}.{index}.1.fastq.gz", index = batch_to_samples.loc['L002']['sample'])),
      read2 = temp(expand("demux/L002_split.{{ss}}.{index}.2.fastq.gz",  index = batch_to_samples.loc['L002']['sample'])),
    resources:
     mem = 8,
     runtime = 24
    wildcard_constraints:
       ss = "[0-9]+"
    params:
     outprefix = "demux/L002_split.{ss}",
     log = "demux/L002_split.{ss}.log",
     ref = {REF}
    shell:
     """
         {DEMUX} -1 {input.read1} -2 {input.read2} -p {input.sample_sheet} -o {params.outprefix} > {params.log}
     """

rule DemuxL3:
    input:
      read1 = "/broad/hptmp/ruolin/L003_split_r1.{ss}.fastq",
      read2 = "/broad/hptmp/ruolin/L003_split_r2.{ss}.fastq",
      sample_sheet = lambda wildcards: batch_to_samplesheet.loc['L003']['sample_sheet']
    output:
      read1 = temp(expand("demux/L003_split.{{ss}}.{index}.1.fastq.gz", index = batch_to_samples.loc['L003']['sample'])),
      read2 = temp(expand("demux/L003_split.{{ss}}.{index}.2.fastq.gz",  index = batch_to_samples.loc['L003']['sample'])),
    resources:
     mem = 8,
     runtime = 24
    wildcard_constraints:
       ss = "[0-9]+"
    params:
     outprefix = "demux/L003_split.{ss}",
     log = "demux/L003_split.{ss}.log",
     ref = {REF}
    shell:
     """
         {DEMUX} -1 {input.read1} -2 {input.read2} -p {input.sample_sheet} -o {params.outprefix} > {params.log}
     """

rule DemuxL4:
    input:
      read1 = "/broad/hptmp/ruolin/L004_split_r1.{ss}.fastq",
      read2 = "/broad/hptmp/ruolin/L004_split_r2.{ss}.fastq",
      sample_sheet = lambda wildcards: batch_to_samplesheet.loc['L004']['sample_sheet']
    output:
      read1 = temp(expand("demux/L004_split.{{ss}}.{index}.1.fastq.gz", index = batch_to_samples.loc['L004']['sample'])),
      read2 = temp(expand("demux/L004_split.{{ss}}.{index}.2.fastq.gz",  index = batch_to_samples.loc['L004']['sample'])),
    resources:
     mem = 8,
     runtime = 24
    wildcard_constraints:
       ss = "[0-9]+"
    params:
     outprefix = "demux/L004_split.{ss}",
     log = "demux/L004_split.{ss}.log",
     ref = {REF}
    shell:
     """
         {DEMUX} -1 {input.read1} -2 {input.read2} -p {input.sample_sheet} -o {params.outprefix} > {params.log}
     """

# rule GzipLog:
#      input:
#          expand("demux/{batch_id}_split.{{ss}}.log", batch_id = batch_ids)
#      output:
#          expand("demux/{batch_id}_split.{{ss}}.log.gz", batch_id = batch_ids)
#      shell:
#         """
#         gzip {input}
#         """

rule Trim:
    input:
        read1 = "demux/{batch_id}_split.{ss}.{index}.1.fastq.gz",
        read2 = "demux/{batch_id}_split.{ss}.{index}.2.fastq.gz",
    output:
        highconf = temp("adap_trimmed/{batch_id}_split.{ss}.{index}.highconf.bam"),
        log = temp("adap_trimmed/{batch_id}_split.{ss}.{index}.trim.log"),
    params:
        adap_type = "custom_primer_v2",
        outprefix = "adap_trimmed/{batch_id}_split.{ss}.{index}",
        rgsm = "{index}"
    resources:
        mem = 8,
        runtime = 24
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+",
        ss = "[0-9]+"
    shell:
        """
            {TRIM} -1 {input.read1} -2 {input.read2} -o  {params.outprefix} -u 3 -U 3 -f 2 -t 2 -s {params.rgsm} > {output.log} 
        """

rule AlignRawTrimmed:
    input:
         "adap_trimmed/{batch_id}_split.{ss}.{index}.{type}.bam",
    output:
          tmp = temp("tmp/{batch_id}_split.{ss}.{index}.{type}.aligned_tmp.sam"),
    params:
          reference = REF,
    resources:
             mem = 8,
             ncores = 1,
             runtime = 48
    wildcard_constraints:
          index = "[0-9a-zA-Z_-]+"
    shell:
         """
         samtools fastq {input} | \
         bwa mem \
             -K 100000000 \
             -t {resources.ncores} \
             -p \
             -Y \
             {params.reference} - > {output.tmp}
         """

rule ZipperBamAlignment:
    input:
        mapped = "tmp/{batch_id}_split.{ss}.{index}.{type}.aligned_tmp.sam",
        unmapped = "adap_trimmed/{batch_id}_split.{ss}.{index}.{type}.bam"
    output:
        bam = temp("tmp/{batch_id}_split.{ss}.{index}.{type}.aligned.bam"),
        bai = temp("tmp/{batch_id}_split.{ss}.{index}.{type}.aligned.bam.bai")
    params:
        reference = REF,
    resources:
        mem = 4,
        runtime = 96,
        ncores = 2
    shell:
        """
        {FGBIO} --compression 0 --async-io ZipperBams \
            -i {input.mapped} \
            --unmapped {input.unmapped} \
             --ref {params.reference} \
        | samtools sort - -o {output.bam} -O BAM -@ {resources.ncores} && samtools index {output.bam} -@ {resources.ncores}
        """

rule MergeSplit:
    input:
         bam = expand("tmp/{{batch_id}}_split.{ss}.{{index}}.highconf.aligned.bam", ss=SPLITS),
         bai = expand("tmp/{{batch_id}}_split.{ss}.{{index}}.highconf.aligned.bam.bai", ss=SPLITS)
    output:
          bam = temp("tmp/{batch_id}.{index}.raw.aligned.bam"),
          bai = temp("tmp/{batch_id}.{index}.raw.aligned.bam.bai")
    resources:
          mem = 8,
          runtime = 72,
          ncores = config['ncores']
    shell:
         """
         samtools merge -@ {resources.ncores} {output.bam} {input.bam} && samtools index {output.bam} -@ {resources.ncores}
         """

rule MergeLogSplit:
    input:
         expand(ADAPTER_TRIM_OUT + "/{{batch_id}}_split.{ss}.{{index}}.trim.log", ss=SPLITS),
    output:
         ADAPTER_TRIM_OUT + "/{batch_id}.{index}.trim.log"
    wildcard_constraints:
          batch_id = "[0-9a-zA-Z_-]+",
          index = "[0-9a-zA-Z_-]+"
    resources:
          mem = 8,
          tuntime = 24,
          ncores = 1
    shell:
         """
          {AGG_LOG_SCRIPT} {input} {output}
         """

rule SortBam:
    input:
        "tmp/{batch_id}.{index}.raw.aligned.bam"
    output:
        temp("tmp/{batch_id}.{index}.raw.aligned.sortbyname.bam")
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+"
    params:
        temp = temp("/broad/hptmp/{batch_id}.{index}.raw.aligned.sortbyname")
    resources:
        mem = 8,
        runtime = 48,
    shell:
        """
        samtools sort -n {input} -o {output} -T {params.temp}
        """


rule CDSByProduct:
    input:
         trim_log = ADAPTER_TRIM_OUT + "/{batch_id}.{index}.trim.log",
         highconf_bam = "tmp/{batch_id}.{index}.raw.aligned.sortbyname.bam",
    output:
          met = Metrics_OUT + "/byproduct/{batch_id}.{index}.byproduct.txt"
    params:
          sid = "{index}"
    wildcard_constraints:
          index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 24,
        runtime = 24,
    shell:
         """
            {AGG_METRICS_SCRIPT} --sample_id {params.sid} --trim_log {input.trim_log} \
            --highconf_bam {input.highconf_bam} > {output.met}
         """

rule MergeRawGroup:
    input:
         lambda wildcard: expand("tmp/{batch_id}.{{index}}.raw.aligned.bam", batch_id = sample_to_batch.loc[wildcard.index]['batch'])
    output:
          bam = temp("tmp/{index}.raw.aligned.bam")
    resources:
             mem = 8,
             ncores = 1,
             runtime = 72,
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+"
    run:
        if len(input) == 1:
            shell("cp {input} {output.bam}")
        else:
            shell("samtools merge -@ {resources.ncores} {output.bam} {input}")

rule ReplaceRawReadGroup:
    input:
       "tmp/{index}.raw.aligned.bam"
    output:
       bam = temp("tmp/{index}.raw.replacerg.bam"),
       bai = temp("tmp/{index}.raw.replacerg.bai"),
    params:
        sid = "{index}"
    resources:
        mem = 8,
        runtime = 32,
    shell:
        """
        {PICARD} AddOrReplaceReadGroups \
           I={input} \
           O={output.bam} \
           CREATE_INDEX=true \
           RGID=4 \
           RGLB=lib1 \
           RGPL=ILLUMINA \
           RGPU=unit1 \
           RGSM={params.sid}
        """

rule MarkRawDuplicates:
    input:
        bam = "tmp/{index}.raw.replacerg.bam"
    resources:
        mem = 32,
        runtime = 48,
    output:
        bam = "tmp/{index}.raw.replacerg.markdup.bam",
        met = "metrics/{index}.raw.marked_duplicates.txt",
        umi = "metrics/{index}.raw.umi_metrics.txt"
    shell:
        """
        {PICARD} UmiAwareMarkDuplicatesWithMateCigar I={input.bam} O={output.bam} M={output.met} UMI_METRICS={output.umi} \
            CREATE_INDEX=true \
            CLEAR_DT=false DUPLEX_UMI=true BARCODE_TAG=RX \
            TAG_DUPLICATE_SET_MEMBERS=true TAGGING_POLICY=All PG=null
        """


rule CollectInsertSizeMetrics:
    input:
         bam = "tmp/{index}.raw.replacerg.markdup.bam",
    output:
          txt = Metrics_OUT + "/{index}.raw.insert_size_metrics.txt",
          hist = Metrics_OUT + "/{index}.raw.insert_size_histogram.pdf"
    shell:
         """
         {PICARD} CollectInsertSizeMetrics I={input.bam} O={output.txt} H={output.hist} M=0.5 W=600 DEVIATIONS=100
         """

#rule DuplexConsensus:
#    input:
#        "tmp/{batch_id}_split.{ss}.{index}.highconf.aligned.bam",
#    output:
#         tmp = temp("tmp/{batch_id}_split.{ss}.{index}.consensus.tmp.bam"),
#         bam = temp("consensus/{batch_id}_split.{ss}.{index}.consensus.bam")
#    params:
#         tmpdir = "tmp"
#    resources:
#         mem = 8,
#         runtime = 72,
#         ncores = 1,
#    shell:
#         """
#         {CONSENSUS_BIN} -b {input} -o {output.tmp} -m 0 -q 0 -p 1 -d {params.tmpdir} -T {resources.ncores} &&
#         {PICARD} SortSam I={output.tmp} O={output.bam} SO=queryname TMP_DIR=./tmp
#         """
#
#rule AlignConsensus:
#    input:
#         "consensus/{batch_id}_split.{ss}.{index}.consensus.bam"
#    output:
#         fastq1 = temp("tmp/{batch_id}_split.{ss}.{index}.1.fq"),
#         fastq2 = temp("tmp/{batch_id}_split.{ss}.{index}.2.fq"),
#         tmp = temp("tmp/{batch_id}_split.{ss}.{index}.aligned_tmp.bam"),
#         bam = temp("consensus/{batch_id}_split.{ss}.{index}.cds_consensus.bam"),
#         bai = temp("consensus/{batch_id}_split.{ss}.{index}.cds_consensus.bai")
#    params:
#          reference = REF,
#    resources:
#             mem = 8,
#             ncores = 1,
#             runtime = 48
#    shell:
#         """
#         samtools fastq {input} | \
#         bwa mem -p \
#             -K 100000000 \
#             -Y \
#             -t {resources.ncores} \
#             {params.reference} {output.fastq1} {output.fastq2} > {output.tmp} &&
#         {PICARD} MergeBamAlignment \
#             ALIGNED={output.tmp} \
#             UNMAPPED={input} \
#             O={output.bam} \
#             R={params.reference} \
#             TMP_DIR={tmpdir} \
#             CREATE_INDEX=true \
#             MAX_RECORDS_IN_RAM=5000000
#         """
#
#rule MergeDSCSplit:
#    input:
#        bam = expand("consensus/{{batch_id}}_split.{ss}.{{index}}.cds_consensus.bam", ss=SPLITS),
#        bai = expand("consensus/{{batch_id}}_split.{ss}.{{index}}.cds_consensus.bai", ss=SPLITS)
#    output:
#        bam = temp("tmp/{batch_id}.{index}.cds_consensus.aligned.bam"),
#        bai = temp("tmp/{batch_id}.{index}.cds_consensus.aligned.bam.bai")
#    resources:
#        mem = 8,
#        runtime = 72,
#        ncores = config['ncores']
#    shell:
#        """
#        samtools merge -@ {resources.ncores} {output.bam} {input.bam} && samtools index {output.bam} -@ {resources.ncores}
#        """
#
#rule MergeDSCGroup:
#    input:
#        lambda wildcard: expand("tmp/{batch_id}.{{index}}.cds_consensus.aligned.bam", batch_id = sample_to_batch.loc[wildcard.index]['batch'])
#    output:
#        bam = temp("tmp/{index}.cds_consensus.aligned.bam")
#    resources:
#        mem = 8,
#        ncores = 1,
#        runtime = 72,
#    wildcard_constraints:
#        index = "[0-9a-zA-Z_-]+"
#    run:
#        if len(input) == 1:
#            shell("cp {input} {output.bam}")
#        else:
#            shell("samtools merge -@ {resources.ncores} {output.bam} {input}")

#rule ReplaceDSCReadGroup:
#    input:
#        "tmp/{index}.cds_consensus.aligned.bam"
#    output:
#        bam = temp("tmp/{index}.cds_consensus.replacerg.bam"),
#        bai = temp("tmp/{index}.cds_consensus.replacerg.bai"),
#    params:
#        sid = "{index}"
#    resources:
#        mem = 8,
#        runtime = 72,
#    shell:
#        """
#        {PICARD} AddOrReplaceReadGroups \
#           I={input} \
#           O={output.bam} \
#           CREATE_INDEX=true \
#           RGID=4 \
#           RGLB=lib1 \
#           RGPL=ILLUMINA \
#           RGPU=unit1 \
#           RGSM={params.sid}
#        """
#
#rule MarkDuplicates:
#    input:
#        bam = "tmp/{index}.cds_consensus.replacerg.bam"
#    resources:
#        mem = 36,
#        runtime = 48,
#    output:
#        bam = temp("consensus/{index}.cds_consensus.replacerg.markdup.bam"),
#        bai = temp("consensus/{index}.cds_consensus.replacerg.markdup.bai"),
#        met = "metrics/{index}.marked_duplicates.txt"
#    shell:
#        """
#        {PICARD} MarkDuplicates I={input.bam} O={output.bam} M={output.met} CREATE_INDEX=true \
#            TAG_DUPLICATE_SET_MEMBERS=true TAGGING_POLICY=All PG=null BARCODE_TAG=RX DUPLEX_UMI=true
#        """

rule GroupReadByUMI:
    input:
         "tmp/{index}.raw.replacerg.bam"
         #"tmp/{index}.cds_consensus.aligned.bam",
    output:
         bam = "groupbyumi/{index}.GroupedByUmi.bam",
         histogram = Metrics_OUT + "/{index}.umiHistogram.txt"
    resources:
             mem = 8,
             runtime = 32,
             ncores = 2
    shell:
         """
         {FGBIO} --compression 1 --async-io \
              GroupReadsByUmi \
             -i {input} \
             -o {output.bam} \
             -f {output.histogram} \
             -m 0 \
             --strategy=paired
         """

rule FgbioCollapseReadFamilies:
    input:
         "groupbyumi/{index}.GroupedByUmi.bam",
    output:
         temp("tmp/{index}.mol_consensus.bam")
    params:
          rg = "{index}"
    resources:
             mem = 8,
             runtime = 48,
             ncores = config['ncores']
    shell:
         """

         {FGBIO} --compression 1 CallMolecularConsensusReads \
             -i {input} \
             -o {output} \
             -p {params.rg} \
             --threads {resources.ncores} \
             --consensus-call-overlapping-bases false \
             -M 1
         """

rule AlignMolecularConsensusReads:
    input:
         "tmp/{index}.mol_consensus.bam"
    output:
          temp("tmp/{index}.mol_consensus.aligned.sam")
    params:
          reference = REF,
    resources:
             mem = 6,
             runtime = 96,
             ncores = 4
    shell:
         """
         samtools fastq {input} \
         | {BWA} mem -K 100000000 -t {resources.ncores} -p -Y {params.reference} - > {output} 
         """

rule MergeAndSortMoleculeConsensusReads:
    input:
        mapped = "tmp/{index}.mol_consensus.aligned.sam",
        unmapped =  "tmp/{index}.mol_consensus.bam"
    output:
        bam = "consensus/{index}.mol_consensus.aligned.bam",
        bai = "consensus/{index}.mol_consensus.aligned.bam.bai"
    params:
        reference = REF,
    resources:
        mem = 4,
        runtime = 96,
        ncores = config['ncores']
    shell:
        """
        {FGBIO} --compression 0 --async-io ZipperBams \
            -i {input.mapped} \
            --unmapped {input.unmapped} \
             --ref {params.reference} \
             --tags-to-reverse Consensus \
             --tags-to-revcomp Consensus \
        | samtools sort - -o {output.bam} -O BAM -@ {resources.ncores} && samtools index {output.bam} -@ {resources.ncores}
        """
